{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "737373c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpi4py import MPI\n",
    "import pandas as pd\n",
    "import ijson\n",
    "from collections import Counter, defaultdict\n",
    "import time\n",
    "\n",
    "\n",
    "state_dict = {\n",
    "    '1': 'New South Wales',\n",
    "    '2': 'Victoria',\n",
    "    '3': 'Queensland',\n",
    "    '4': 'South Australia',\n",
    "    '5': 'Western Australia',\n",
    "    '6': 'Tasmania',\n",
    "    '7': 'Northern Territory',\n",
    "    '8': 'Australian Capital Territory',\n",
    "    '9': 'Other Territories'\n",
    "}\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "size = comm.Get_size()\n",
    "rank = comm.Get_rank()\n",
    "\n",
    "\n",
    "def load_twitter_data(file):\n",
    "    with open(file, 'r', encoding='utf-8') as file:\n",
    "        items = ijson.items(file, 'item')\n",
    "        data = []\n",
    "        for item in items:\n",
    "            tweet = item['data']\n",
    "            location = item['includes']['places'][0]\n",
    "            full_name = location['full_name'].split(', ')\n",
    "            data.append({\n",
    "                'author_id': tweet['author_id'],\n",
    "                'suburb': full_name[0],\n",
    "                'state': full_name[1] if len(full_name) > 1 else '',\n",
    "            })\n",
    "        data = pd.DataFrame(data)\n",
    "        data = data.dropna(subset=['author_id'])\n",
    "        return data\n",
    "\n",
    "\n",
    "def load_sal_data(file):\n",
    "    data = []\n",
    "    with open(file, 'r', encoding='utf-8') as file:\n",
    "        suburbs_data = ijson.items(file, '')\n",
    "        for suburbs in suburbs_data:\n",
    "            for suburb, values in suburbs.items():\n",
    "                data.append({\n",
    "                    'suburb': suburb,\n",
    "                    'ste': values['ste'],\n",
    "                    'gcc': values['gcc'],\n",
    "                    'sal': values['sal'],\n",
    "                    'ste_name': state_dict[values['ste']]\n",
    "                })\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "\n",
    "def analyze(twitter_data, sal_data):\n",
    "    res = {\n",
    "        'tweets_count': 0,\n",
    "        'top_users': Counter(),\n",
    "        'cities_users': defaultdict(set)\n",
    "    }\n",
    "    for _, tweet in twitter_data.iterrows():\n",
    "        author_id = tweet['author_id']\n",
    "        suburb = tweet['suburb']\n",
    "        state = tweet['state']\n",
    "        matched_suburb = sal_data[(sal_data['suburb'].str.lower() == suburb.lower()) &\n",
    "                                  (sal_data['ste_name'].str.lower() == state.lower())]\n",
    "        if not matched_suburb.empty:\n",
    "            gcc = matched_suburb['gcc'].iloc[0]\n",
    "            # Ignore rural areas\n",
    "            if not gcc.startswith(\"1r\"):\n",
    "                res['tweets_count'] += 1\n",
    "                res['top_users'][author_id] += 1\n",
    "                res['cities_users'][author_id].add(gcc)\n",
    "    return res\n",
    "\n",
    "\n",
    "def parallel_processing(data):\n",
    "    chunk_size = len(data) // size\n",
    "    start = rank * chunk_size\n",
    "    if rank != size - 1:\n",
    "        end = (rank + 1) * chunk_size\n",
    "    else:\n",
    "        end = len(data)\n",
    "    return data[start:end]\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "d2e3802b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tweets in Greater Capital cities: 524\n",
      "\n",
      "Top 10 tweeters\n",
      "#1 51378153: 32 tweets\n",
      "#2 384233102: 23 tweets\n",
      "#3 156677140: 17 tweets\n",
      "#4 213903403: 11 tweets\n",
      "#5 4648031797: 9 tweets\n",
      "#6 99367063: 9 tweets\n",
      "#7 7050962: 7 tweets\n",
      "#8 935936493201305601: 7 tweets\n",
      "#9 30839139: 7 tweets\n",
      "#10 20522796: 6 tweets\n",
      "\n",
      "Top 10 tweeters making tweets from the most different locations\n",
      "#1 167824089: {'1gsyd'}\n",
      "#2 384233102: {'1gsyd'}\n",
      "#3 1951324189: {'1gsyd'}\n",
      "#4 156677140: {'1gsyd'}\n",
      "#5 276736970: {'1gsyd'}\n",
      "#6 7050962: {'1gsyd'}\n",
      "#7 286074748: {'1gsyd'}\n",
      "#8 1363631988: {'1gsyd'}\n",
      "#9 273192130: {'1gsyd'}\n",
      "#10 450993384: {'1gsyd'}\n"
     ]
    }
   ],
   "source": [
    "    twitter_data = load_twitter_data('data/twitter-data-small.json')\n",
    "    sal_data = load_sal_data('data/sal.json')\n",
    "\n",
    "    twitter_data_chunk = parallel_processing(twitter_data)\n",
    "\n",
    "    batch = analyze(twitter_data_chunk, sal_data)\n",
    "    results = comm.gather(batch, root=0)\n",
    "\n",
    "    if rank == 0:\n",
    "        stat = {\n",
    "            'tweets_count': 0,\n",
    "            'top_users': Counter(),\n",
    "            'cities_users': defaultdict(set)\n",
    "        }\n",
    "\n",
    "        for result in results:\n",
    "            stat['tweets_count'] += result['tweets_count']\n",
    "            stat['top_users'] += result['top_users']\n",
    "\n",
    "            for author_id, cities in result['cities_users'].items():\n",
    "                stat['cities_users'][author_id].update(cities)\n",
    "\n",
    "        top_users = stat['top_users'].most_common(10)\n",
    "        most_cities_users = sorted(stat['cities_users'].items(), key=lambda x: len(x[1]), reverse=True)[:10]\n",
    "\n",
    "        print(\"Number of tweets in Greater Capital cities:\", stat['tweets_count'])\n",
    "\n",
    "        print(\"\\nTop 10 tweeters\")\n",
    "        cnt = 1\n",
    "        for author_id, count in top_users:\n",
    "            print(f\"#{cnt} {author_id}: {count} tweets\")\n",
    "            cnt += 1\n",
    "\n",
    "        cnt = 1\n",
    "        print(\"\\nTop 10 tweeters making tweets from the most different locations\")\n",
    "        for author_id, cities in most_cities_users:\n",
    "            print(f\"#{cnt} {author_id}: {cities}\")\n",
    "            cnt += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "67efb6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_data_chunk.suburb = twitter_data_chunk.suburb.apply(str.lower)\n",
    "twitter_data_chunk_new = pd.merge(twitter_data_chunk, sal_data[['suburb', 'gcc']], on='suburb')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "80d0f21e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1gsyd    216\n",
       "2gmel    129\n",
       "5gper     71\n",
       "1rnsw     53\n",
       "3gbri     46\n",
       "4gade     20\n",
       "2rvic     14\n",
       "8acte     14\n",
       "6ghob      7\n",
       "3rqld      4\n",
       "Name: gcc, dtype: int64"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter_data_chunk_new.gcc.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "5270715e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51378153              32\n",
      "384233102             23\n",
      "156677140             17\n",
      "213903403             11\n",
      "99367063               9\n",
      "4648031797             9\n",
      "30839139               7\n",
      "935936493201305601     7\n",
      "7050962                7\n",
      "20522796               6\n",
      "Name: author_id, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "twitter_data_chunk_new.author_id.value_counts()\n",
    "print(twitter_data_chunk_new.author_id.value_counts().head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "69e310ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1002522913235300352    1\n",
       "4185121399             1\n",
       "401237384              1\n",
       "388129974              1\n",
       "384233102              1\n",
       "38359893               1\n",
       "38177628               1\n",
       "378503524              1\n",
       "36822807               1\n",
       "366629853              1\n",
       "Name: author_id, dtype: int64"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter_unique_gcc_count = twitter_data_chunk_new.groupby([\"author_id\", \"gcc\"]).size().reset_index(name=\"nTweets\")\n",
    "twitter_unique_gcc_count.author_id.value_counts().head(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
